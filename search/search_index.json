{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Overview","text":"<p><code>ALineMol</code> is a Python library designed to assist in drug discovery by providing powerful methods for estimating the out-of-distribution (OOD) performance of molecular machine learning models, including both classical machine learning (RF, XGB, etc) and graph neural networks (GNNs). Built on top of the popular PyTorch library, <code>ALineMol</code> offers a simple, user-friendly API for assessing OOD performance. It is designed to be flexible and easy to integrate into existing workflows.</p> <p>The library first generates OOD data based on various splitting strategies, then benchmarks and evaluates the performance of different models on this OOD data. This approach helps estimate the generalization power and robustness of models to OOD data.</p>"},{"location":"index.html#installation","title":"Installation","text":"<p><code>ALineMol</code> can be installed using pip. First, clone this repository, create a new conda environment with the required packages, and finally, install the repository using pip.</p> <pre><code>conda env create -f environment.yml\nconda activate alinemol\n\npip install --no-deps -e .\n</code></pre>"},{"location":"api/models.html","title":"<code>alinemol.models</code>","text":""},{"location":"api/models.html#alinemol.models","title":"alinemol.models","text":""},{"location":"api/preprocessing.html","title":"<code>alinemol.preprocessing</code>","text":""},{"location":"api/preprocessing.html#alinemol.preprocessing","title":"alinemol.preprocessing","text":""},{"location":"api/preprocessing.html#alinemol.preprocessing.standardize_smiles","title":"standardize_smiles","text":"<pre><code>standardize_smiles(\n    x: DataFrame, taut_canonicalization: bool = True\n) -&gt; pd.DataFrame\n</code></pre> <p>Standardization of a SMILES string.</p> <p>Uses the <code>Standardizer</code> to perform sequence of cleaning operations on a SMILES string.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>pd.DataFrame with <code>smiles</code> column</p> required <code>taut_canonicalization</code> <code>bool</code> <p>whether or not to use tautomer canonicalization</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame with 'canonical_smiles', 'molecular_weight', and 'num_atoms' additional columns</p>"},{"location":"api/preprocessing.html#alinemol.preprocessing.drop_duplicates","title":"drop_duplicates","text":"<pre><code>drop_duplicates(x: DataFrame) -&gt; pd.DataFrame\n</code></pre> <p>Remove conflicting duplicates from a DataFrame.</p> This function processes the DataFrame to <ul> <li>Drop rows where the 'canonical_smiles' values are the same but the labels differ (conflicting rows).</li> <li>Retain only one row for each set of identical 'canonical_smiles' values with the same label.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>The input DataFrame containing a 'canonical_smiles' column.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with conflicting duplicates removed, ensuring unique rows.</p>"},{"location":"api/preprocessing.html#alinemol.preprocessing.standardization_pipeline","title":"standardization_pipeline","text":"<pre><code>standardization_pipeline(\n    x: DataFrame, taut_canonicalization: bool = True\n) -&gt; pd.DataFrame\n</code></pre> <p>Standardization pipeline for a DataFrame.</p> This function performs the following operations on the input DataFrame <ul> <li>Standardize the 'smiles' column using the <code>standardize_smiles</code> function.</li> <li>Drop conflicting duplicates using the <code>drop_duplicates</code> function.</li> <li>Return a DataFrame with 'smiles' and 'label' columns.</li> </ul> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>The input DataFrame containing a 'smiles' column.</p> required <code>taut_canonicalization</code> <code>bool</code> <p>Whether or not to use tautomer canonicalization.</p> <code>True</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame with standardized 'canonical_smiles' values and conflicting duplicates removed.</p> Note <p>The input DataFrame must contain a 'smiles' and <code>label</code> column. Output DataFrame will contain 'smiles' and <code>label</code> columns.</p>"},{"location":"api/splitters.html","title":"<code>alinemol.splitters</code>","text":""},{"location":"api/splitters.html#alinemol.splitters","title":"alinemol.splitters","text":""},{"location":"api/splitters.html#alinemol.splitters.MolecularLogPSplit","title":"MolecularLogPSplit","text":"<p>               Bases: <code>BaseShuffleSplit</code></p> <p>Split a molecular dataset by sorting molecules according to their LogP values.</p> <p>This splitter is designed for chemical domain shift experiments, where you want to evaluate how well models generalize to molecules with different physical properties than those they were trained on. LogP (octanol-water partition coefficient) is a measure of lipophilicity, which affects molecular solubility, permeability, and binding properties.</p> <p>The splitter works by: 1. Calculating LogP values for all molecules 2. Sorting molecules by their LogP values 3. Splitting the sorted list according to train/test size parameters</p> <p>When generalize_to_larger=True (default), the training set contains molecules with lower LogP values, and the test set contains those with higher LogP values. This mimics the real-world scenario of testing on molecules with properties outside the training distribution.</p> <p>Parameters:</p> Name Type Description Default <code>generalize_to_larger</code> <p>bool, default=True If True, train set will have smaller LogP values, test set will have larger values. If False, train set will have larger LogP values, test set will have smaller values.</p> <code>True</code> <code>n_splits</code> <p>int, default=5 Number of re-shuffling &amp; splitting iterations. Note that for this deterministic splitter, all iterations will produce the same split.</p> <code>5</code> <code>smiles</code> <p>List[str], optional List of SMILES strings if not provided directly as input in split() or _iter_indices(). Useful when the input X to those methods is not a list of SMILES strings but some other feature representation.</p> <code>None</code> <code>test_size</code> <p>float or int, optional If float, represents the proportion of the dataset to include in the test split. If int, represents the absolute number of test samples. If None, the value is set to the complement of the train size.</p> <code>None</code> <code>train_size</code> <p>float or int, optional If float, represents the proportion of the dataset to include in the train split. If int, represents the absolute number of train samples. If None, the value is automatically set to the complement of the test size.</p> <code>None</code> <code>random_state</code> <p>int or RandomState instance, optional Controls the randomness of the training and testing indices produced. Note that this splitter is deterministic, so random_state only affects the implementation of _validate_shuffle_split.</p> <code>None</code> <p>Examples:</p> <pre><code>&gt;&gt;&gt; from alinemol.splitters import MolecularLogPSplit\n&gt;&gt;&gt; import numpy as np\n&gt;&gt;&gt; # Example with list of SMILES\n&gt;&gt;&gt; smiles = [\"CCO\", \"CC(=O)O\", \"c1ccccc1\", \"CCN\", \"CCCCCCC\"]\n&gt;&gt;&gt; splitter = MolecularLogPSplit(generalize_to_larger=True, test_size=0.4)\n&gt;&gt;&gt; for train_idx, test_idx in splitter.split(smiles):\n...     print(f\"Training on: {[smiles[i] for i in train_idx]}\")\n...     print(f\"Testing on: {[smiles[i] for i in test_idx]}\")\n...     break  # Just show the first split\n</code></pre> <pre><code>&gt;&gt;&gt; # Example with separate features and target\n&gt;&gt;&gt; X = np.random.randn(5, 10)  # Some molecular features\n&gt;&gt;&gt; y = np.random.randint(0, 2, 5)  # Binary target\n&gt;&gt;&gt; splitter = MolecularLogPSplit(smiles=smiles, test_size=0.4)\n&gt;&gt;&gt; for train_idx, test_idx in splitter.split(X, y):\n...     X_train, X_test = X[train_idx], X[test_idx]\n...     y_train, y_test = y[train_idx], y[test_idx]\n...     break  # Just show the first split\n</code></pre> Notes <ul> <li>LogP values are calculated using the Crippen method implemented in datamol</li> <li>This splitter is deterministic - calling split() multiple times will   produce the same split regardless of n_splits value</li> <li>Useful for testing model extrapolation to molecules with different   physical-chemical properties than the training set</li> </ul>"},{"location":"api/splitters.html#alinemol.splitters.StratifiedRandomSplit","title":"StratifiedRandomSplit","text":"<p>               Bases: <code>object</code></p> <p>Randomly reorder datasets and then split them. make sure that the label distribution among the training, validation and test sets are the same as the original dataset.</p> <p>The dataset is split with permutation and the splitting is hence stratified random.</p>"},{"location":"api/splitters.html#alinemol.splitters.StratifiedRandomSplit.train_val_test_split","title":"train_val_test_split  <code>staticmethod</code>","text":"<pre><code>train_val_test_split(\n    dataset: LabeledDataset,\n    frac_train: float = 0.8,\n    frac_val: float = 0.1,\n    frac_test: float = 0.1,\n    random_state: RandomStateType = None,\n) -&gt; DatasetSplit\n</code></pre> <p>Randomly permute the dataset and then stratified split it into three consecutive chunks for training, validation and test.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>LabeledDataset We assume <code>len(dataset)</code> gives the size for the dataset and <code>dataset[i]</code> gives the ith datapoint.</p> required <code>frac_train</code> <p>float Fraction of data to use for training. By default, we set this to be 0.8, i.e. 80% of the dataset is used for training.</p> <code>0.8</code> <code>frac_val</code> <p>float Fraction of data to use for validation. By default, we set this to be 0.1, i.e. 10% of the dataset is used for validation.</p> <code>0.1</code> <code>frac_test</code> <p>float Fraction of data to use for test. By default, we set this to be 0.1, i.e. 10% of the dataset is used for test.</p> <code>0.1</code> <code>random_state</code> <p>None, int or array_like, optional Random seed used to initialize the pseudo-random number generator. Can be any integer between 0 and 2**32 - 1 inclusive, an array (or other sequence) of such integers, or None (the default). If seed is None, then RandomState will try to read data from /dev/urandom (or the Windows analogue) if available or seed from the clock otherwise.</p> <code>None</code> <p>Returns:</p> Type Description <code>DatasetSplit</code> <p>list of length 3 Subsets for training, validation and test, which also have <code>len(dataset)</code> and <code>dataset[i]</code> behaviors.</p>"},{"location":"api/splitters.html#alinemol.splitters.StratifiedRandomSplit.k_fold_split","title":"k_fold_split  <code>staticmethod</code>","text":"<pre><code>k_fold_split(\n    dataset: LabeledDataset,\n    k: int = 5,\n    random_state: RandomStateType = None,\n    log: bool = True,\n) -&gt; KFoldSplit\n</code></pre> <p>Performs stratified k-fold split of the dataset.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <p>LabeledDataset We assume <code>len(dataset)</code> gives the size for the dataset and <code>dataset[i]</code> gives the ith datapoint. The dataset should have a 'labels' attribute.</p> required <code>k</code> <p>int Number of folds. Default is 5.</p> <code>5</code> <code>random_state</code> <p>None, int or array_like, optional Random seed used to initialize the pseudo-random number generator. Can be any integer between 0 and 2**32 - 1 inclusive, an array (or other sequence) of such integers, or None (the default).</p> <code>None</code> <code>log</code> <p>bool Whether to log information about the split. Default is True.</p> <code>True</code> <p>Returns:</p> Type Description <code>KFoldSplit</code> <p>list of tuples Each tuple contains (train_set, val_set) where train_set and val_set are Subset objects of the original dataset.</p>"},{"location":"api/splitters.html#alinemol.splitters.UMAPSplit","title":"UMAPSplit","text":"<p>               Bases: <code>GroupShuffleSplit</code></p> <p>Group-based split that uses the UMAP clustering in the input space for splitting.</p> <p>From the following papers: 1. \"UMAP-based clustering split for rigorous evaluation of AI models for virtual screening on cancer cell lines\"     https://doi.org/10.26434/chemrxiv-2024-f1v2v-v2 2. \"On the Best Way to Cluster NCI-60 Molecules\"     https://doi.org/10.3390/biom13030498</p> <p>Parameters:</p> Name Type Description Default <code>n_clusters</code> <code>int</code> <p>The number of clusters to use for clustering</p> <code>10</code> <code>n_neighbors</code> <code>int</code> <p>The number of neighbors to use for the UMAP algorithm</p> <code>100</code> <code>min_dist</code> <code>float</code> <p>The minimum distance between points in the UMAP embedding</p> <code>0.1</code> <code>n_components</code> <code>int</code> <p>The number of components to use for the PCA algorithm</p> <code>2</code> <code>umap_metric</code> <code>Union[str, Callable]</code> <p>The metric to use for the UMAP algorithm</p> <code>'jaccard'</code> <code>linkage</code> <code>str</code> <p>The linkage to use for the AgglomerativeClustering algorithm</p> <code>'ward'</code> <code>n_splits</code> <code>int</code> <p>The number of splits to use for the split</p> <code>5</code> <code>test_size</code> <code>Optional[Union[float, int]]</code> <p>The size of the test set</p> <code>None</code> <code>train_size</code> <code>Optional[Union[float, int]]</code> <p>The size of the train set</p> <code>None</code> <code>random_state</code> <code>Optional[Union[int, RandomState]]</code> <p>The random state to use for the split</p> <code>None</code> Example <p>from alinemol.splitters import UMAPSplit splitter = UMAPSplit(n_clusters=2, linkage=\"ward\", n_neighbors=3, min_dist=0.1, n_components=2, n_splits=5) smiles = [\"c1ccccc1\", \"CCC\", \"CCCC(CCC)C(=O)O\", \"NC1CCCCC1N\",\"COc1cc(CNC(=O)CCCCC=CC(C)C)ccc1O\", \"Cc1cc(Br)c(O)c2ncccc12\", \"OCC(O)c1oc(O)c(O)c1O\"] for train_idx, test_idx in splitter.split(smiles):     print(train_idx)     print(test_idx)</p>"},{"location":"api/splitters.html#alinemol.splitters.LoSplit","title":"LoSplit","text":""},{"location":"api/splitters.html#alinemol.splitters.LoSplit.__init__","title":"__init__","text":"<pre><code>__init__(\n    threshold: float = 0.4,\n    min_cluster_size: int = 5,\n    max_clusters: int = 50,\n    std_threshold: float = 0.6,\n)\n</code></pre> <p>A splitter that prepares data for training ML models for Lead Optimization or to guide molecular generative models. These models must be sensitive to minor modifications of molecules, and this splitter constructs a test that allows the evaluation of a model's ability to distinguish those modifications.</p> <p>Parameters:</p> Name Type Description Default <code>threshold</code> <code>float</code> <p>ECFP4 1024-bit Tanimoto similarity threshold. Molecules more similar than this threshold are considered too similar and can be grouped together in one cluster.</p> <code>0.4</code> <code>min_cluster_size</code> <code>int</code> <p>the minimum number of molecules per cluster.</p> <code>5</code> <code>max_clusters</code> <code>int</code> <p>the maximum number of selected clusters. The remaining molecules go to the training set. This can be useful for limiting your test set to get more molecules in the train set.</p> <code>50</code> <code>std_threshold</code> <code>float</code> <p>the lower bound of the acceptable standard deviation for a cluster's values. It should be greater than the measurement noise. For ChEMBL-like data set it to 0.60 for logKi and 0.70 for logIC50. Set it lower if you have a high-quality dataset.</p> <code>0.6</code> <p>For more information, see a tutorial in the docs and Steshin 2023, Lo-Hi: Practical ML Drug Discovery Benchmark.</p>"},{"location":"api/splitters.html#alinemol.splitters.LoSplit.split","title":"split","text":"<pre><code>split(\n    smiles: List[str], values: List[float], n_jobs: int = -1, verbose: int = 1\n) -&gt; Tuple[List[int], List[List[int]]]\n</code></pre> <p>Split the dataset into test clusters and train.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>List[str]</code> <p>list of SMILES strings representing molecules</p> required <code>values</code> <code>List[float]</code> <p>list of their continuous activity values</p> required <code>n_jobs</code> <code>int</code> <p>number of parallel jobs to run, -1 means use all processors</p> <code>-1</code> <code>verbose</code> <code>int</code> <p>set to 0 to turn off progressbar</p> <code>1</code> <p>Returns:</p> Name Type Description <code>train_idx</code> <code>List[int]</code> <p>list of indices for training set</p> <code>clusters_idx</code> <code>List[List[int]]</code> <p>list of lists containing indices for each cluster</p>"},{"location":"api/splitters.html#alinemol.splitters.HiSplit","title":"HiSplit","text":"<p>               Bases: <code>BaseShuffleSplit</code></p>"},{"location":"api/splitters.html#alinemol.splitters.HiSplit.__init__","title":"__init__","text":"<pre><code>__init__(\n    similarity_threshold: float = 0.4,\n    train_min_frac: float = 0.7,\n    test_min_frac: float = 0.15,\n    coarsening_threshold: Optional[float] = None,\n    verbose: bool = True,\n    max_mip_gap: float = 0.1,\n)\n</code></pre> <p>A splitter that creates train/test splits with no molecules in the test set having ECFP4 Tanimoto similarity greater than similarity_threshold to molecules in the train set.</p> <p>This splitter is designed for evaluating model generalization to structurally dissimilar molecules. It uses a min vertex k-cut algorithm to optimally partition molecules while respecting similarity constraints.</p> <p>Parameters:</p> Name Type Description Default <code>similarity_threshold</code> <code>float</code> <p>ECFP4 Tanimoto threshold. Molecules in the test set won't have a similarity greater than this threshold to those in the train set.</p> <code>0.4</code> <code>train_min_frac</code> <code>float</code> <p>Minimum fraction for the train set, e.g., 0.7 of the entire dataset.</p> <code>0.7</code> <code>test_min_frac</code> <code>float</code> <p>Minimum fraction for the test set, e.g., 0.1 of the entire dataset. It's possible that the k-cut might not be feasible without discarding some molecules, so ensure that the sum of train_min_frac and test_min_frac is less than 1.0.</p> <code>0.15</code> <code>coarsening_threshold</code> <code>Optional[float]</code> <p>Molecules with a similarity greater than the coarsening_threshold will be clustered together. It speeds up execution, but makes the solution less optimal.     None -- Disables clustering (default value).     1.0 -- Won't do anything     0.90 -- will cluster molecules with similarity &gt; 0.90 together</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>If set to False, suppresses status messages.</p> <code>True</code> <code>max_mip_gap</code> <code>float</code> <p>Determines when to halt optimization based on proximity to the optimal solution. For example, setting it to 0.5 yields a faster but less optimal solution, while 0.01 aims for a more optimal solution, potentially at the cost of more computation time.</p> <code>0.1</code>"},{"location":"api/splitters.html#alinemol.splitters.HiSplit.split","title":"split","text":"<pre><code>split(smiles: List[str]) -&gt; Iterator[Tuple[np.ndarray, np.ndarray]]\n</code></pre> <p>Split the dataset into train and test sets such that no molecule in the test has ECFP4 Tanimoto similarity to the train &gt; similarity_threshold.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>List[str]</code> <p>List of SMILES strings representing molecules</p> required <p>Returns:</p> Type Description <code>Iterator[Tuple[ndarray, ndarray]]</code> <p>Tuple containing: - List[int]: Indices of training molecules - List[int]: Indices of test molecules</p> Example <p>from alinemol.splitters.lohi import HiSplit splitter = HiSplit() for train_indices, test_indices in splitter.split(smiles):     print(train_indices)     print(test_indices)</p>"},{"location":"api/splitters.html#alinemol.splitters.HiSplit.k_fold_split","title":"k_fold_split","text":"<pre><code>k_fold_split(\n    smiles: List[str], k: int = 3, fold_min_frac: Optional[float] = None\n) -&gt; List[List[int]]\n</code></pre> <p>Split the dataset into k folds such that no molecule in any fold has an ECFP4 Tanimoto similarity greater than similarity_threshold when compared to molecules in another fold.</p> <p>Parameters:</p> Name Type Description Default <code>smiles</code> <code>List[str]</code> <p>List of SMILES strings representing molecules</p> required <code>k</code> <code>int</code> <p>Number of folds</p> <code>3</code> <code>fold_min_frac</code> <code>Optional[float]</code> <p>Minimum fraction of a fold (e.g., 0.2 of the entire dataset). If not specified (None), it defaults to 0.9 / k.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[int]]</code> <p>List[List[int]]: List of lists, where each list contains the indices of molecules in that fold</p>"},{"location":"api/splitters.html#alinemol.splitters.get_umap_clusters","title":"get_umap_clusters","text":"<pre><code>get_umap_clusters(\n    X: Union[ndarray, List[ndarray]],\n    n_clusters: int = 10,\n    n_neighbors: int = 100,\n    min_dist: float = 0.1,\n    n_components: int = 2,\n    umap_metric: str = \"euclidean\",\n    linkage: str = \"ward\",\n    random_state: Optional[Union[int, RandomState]] = None,\n    n_jobs: int = -1,\n    return_embedding: bool = False,\n    **kwargs\n) -&gt; Union[np.ndarray, Tuple[np.ndarray, np.ndarray]]\n</code></pre> <p>Cluster a list of SMILES strings using the umap clustering algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>Union[ndarray, List[ndarray]]</code> <p>The input data (N * D)</p> required <code>n_clusters</code> <code>int</code> <p>The number of clusters to use for clustering</p> <code>10</code> <code>n_neighbors</code> <code>int</code> <p>The number of neighbors to use for the UMAP algorithm</p> <code>100</code> <code>min_dist</code> <code>float</code> <p>The minimum distance between points in the UMAP embedding</p> <code>0.1</code> <code>n_components</code> <code>int</code> <p>The number of components to use for the PCA algorithm</p> <code>2</code> <code>umap_metric</code> <code>str</code> <p>The metric to use for the UMAP algorithm</p> <code>'euclidean'</code> <code>linkage</code> <code>str</code> <p>The linkage to use for the AgglomerativeClustering algorithm</p> <code>'ward'</code> <code>random_state</code> <code>Optional[Union[int, RandomState]]</code> <p>The random state to use for the PCA algorithm and the Empirical Kernel Map</p> <code>None</code> <code>n_jobs</code> <code>int</code> <p>The number of jobs to use for the UMAP algorithm</p> <code>-1</code> <code>return_embedding</code> <code>bool</code> <p>Whether to return the UMAP embedding</p> <code>False</code> <p>Returns:</p> Type Description <code>Union[ndarray, Tuple[ndarray, ndarray]]</code> <p>Array of cluster labels corresponding to each SMILES string in the input list. If return_embedding is True, returns a tuple of the cluster labels and the UMAP embedding.</p> Example <p>from alinemol.splitters import get_umap_clusters X = np.random.rand(100, 128) clusters_indices, embedding = get_umap_clusters(X, n_clusters=10, n_jobs=1, return_embedding=True) print(clusters_indices)</p>"},{"location":"api/utils.html","title":"<code>alinemol.utils</code>","text":""},{"location":"api/utils.html#alinemol.utils","title":"alinemol.utils","text":""}]}